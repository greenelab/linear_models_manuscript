## Results

### Linear and non-linear models have similar performance in many tasks
We compared the performance of linear and non-linear models across multiple datasets and tasks (fig. @fig:workflow A).
We examined using TPM-normalized RNA-seq data to predict tissue labels from GTEx [@doi:10.1038/ng.2653], tissue labels from Recount3 [@doi:10.1186/s13059-021-02533-6], and metadata-derived sex labels from Flynn et al. [@doi:10.1186/s12859-021-04070-2].
To avoid leakage between cross-validation folds, we placed entire studies into single folds (fig. @fig:workflow B).
We evaluated models on subsampled datasets to determine the extent to which performance was affected by the amount of training data.

![
Schematic of the model analysis workflow. We evaluate three models on multiple classification problems in three datasets (A). We use study-wise splitting by default and evaluate the effects of sample-wise splitting and pre-training (B).
](./images/workflow.svg "Workflow diagram"){#fig:workflow}


We used GTEx [@doi:10.1038/ng.2653] to determine whether linear and non-linear models performed similarly on a well-characterized dataset with consistent experimental protocols across samples.
We first trained our models to differentiate between tissue types on pairs of the five most common tissues in the dataset.
Likely due to the clean nature of the data, all models were able to perform perfectly on these binary classification tasks (fig. @fig:prediction_combined B).
Because binary classification was unable to differentiate between models, we evaluated the models on a more challenging task.
We tested the models on their ability to perform multiclass classification on all 31 tissues present in the dataset.
In the multitask setting, logistic regression slightly outperformed the five-layer neural network, which in turn slightly outperformed the three-layer net (fig. @fig:prediction_combined A).

We then evaluated the same approaches in a dataset with very different characteristics: Sequence Read Archive [@doi:10.1093/nar/gkq1019] samples from Recount3 [@doi:10.1186/s13059-021-02533-6].
We compared the models' ability to differentiate between pairs of tissues (supp. fig. @fig:recount-binary) and found their performance was roughly equivalent.
We also evaluated the models' performance on a multiclass classification problem differentiating between the 21 most common tissues in the dataset.
As in the GTEx setting, the logistic regression model outperformed the five-layer network, which outperformed the three-layer network (fig. @fig:prediction_combined C). 

To examine whether these results held in a problem domain other than tissue type prediction, we tested performance on metadata-derived sex labels (fig. @fig:prediction_combined D), a task previously studied by Flynn et al. [@doi:10.1186/s12859-021-04070-2].
We used the same experimental setup as in our other binary prediction tasks to train the models, but rather than using tissue labels we used sex labels from Flynn et al.
In this setting we found that while the models all performed similarly, the non-linear models tended to have a slight edge over the linear one.

![
Performance of models across four classification tasks. In each panel the loess curve and its 95% confidence interval are plotted based on points from three seeds, ten data subsets, and five folds of study-wise cross-validation (for a total of 150 points per model per panel).
](./images/full_signal_combined.svg ){#fig:prediction_combined}

### There is predictive non-linear signal in transcriptomic data
Our results to this point are consistent with a world where the predictive signal present in transcriptomic data is entirely linear.
If that were the case, non-linear models like neural networks would fail to give any substantial advantage.
However, based on past results we expect there to be relevant nonlinear biological signal [CITE clustermatch].
To get a clearer idea of what that would look like, we simulated three datasets to better understand model performance for a variety of data generating processes.
We created data with both linear and non-linear signal by generating two types of features: half of the features with a linear dividing line between the simulated classes and half with a non-linear dividing line (see [Methods](#methods) for more detail).
After training to classify the simulated dataset, all models effectively predicted the simulated classes.
To determine whether or not there was non-linear signal, we then used Limma [@doi:10.1093/nar/gkv007] to remove the linear signal associated with the endpoint being predicted.
After removing the linear signal from the dataset, non-linear models correctly predicted classes, but logistic regression performed no better than random (fig @fig:simulation).

To confirm that non-linear signal was key to the performance of non-linear methods, we generated another simulated dataset consisting solely of features with a linear dividing line between the classes.
As before, all models were able to predict the different classes well.
However, once the linear signal was removed, all models performed no better than random guessing (fig @fig:simulation).
That the non-linear models only achieved baseline accuracy also indicated that the signal removal method was not injecting non-linear signal into data where non-linear signal did not exist.

We also trained the models on a dataset where all features were Gaussian noise as a negative control.
As expected, the models all performed at baseline accuracy both before and after the signal removal process (fig. @fig:simulation).
This experiment supported our decision to perform signal removal on the training and validation sets separately, as removing the linear signal in the full dataset induced predictive signal (supp. fig. @fig:split-signal-correction).

![
Performance of models in binary classification of simulated data before and after signal removal. Dotted lines indicate expected performance for a naive baseline classifier that predicts the most frequent class.
](./images/simulated_data_combined.svg ){#fig:simulation}

We next removed linear signal from GTEx and Recount3.
We found that the neural nets performed better than the baseline while logistic regression did not (fig. @fig:signal_removed B, supp. fig. @fig:recount-binary-combined).
Similarly, for multiclass problems logistic regression performed poorly while the non-linear models had performance that increased with an increase in data while remaining worse than before the linear signal was removed (fig. @fig:signal_removed A,C).
Likewise, the sex label prediction task showed a marked difference between the neural networks and logistic regression: only the neural networks could learn from the data (fig. @fig:signal_removed D).
In each of the settings, the models performed less well when run on data with signal removed, indicating an increase in the problem's difficulty. 
Logistic regression, in particular, performed no better than random.

![
Performance of models across four classification tasks before and after signal removal
](./images/signal_removed_combined.svg ){#fig:signal_removed}

To verify that our results were not an artifact of our decision to assign studies to cross-validation folds rather than samples, we compared the study-wise splitting that we used with an alternate method called sample-wise splitting.
Sample-wise splitting (see [Methods](#methods)) is common in machine learning, but can leak information between the training and validation sets when samples are not independently and identically distributed among studies - a common feature of data in biology [@doi:10.1038/s41576-021-00434-9].
We found that sample-wise splitting induced substantial performance inflation (supp. fig. @fig:splitting).
The relative performance of each model stayed the same regardless of the data splitting technique, so the results observed were not dependent on the choice of splitting technique.

Another growing strategy in machine learning, especially on biological data where samples are limited, is training models on a general-purpose dataset and fine-tuning them on a dataset of interest.
We examined the performance of models with and without pretraining (supp. fig @fig:pretrain).
We split the Recount3 data into three sets: pretraining, training, and validation (fig. @fig:workflow B), then trained two identically initialized copies of each model.
One was trained solely on the training data, while the other was trained on the pretraining data and fine-tuned on the training data.
The pretrained models showed high performance even when trained with small amounts of data from the training set.
However, the non-linear models did not have a greater performance gain from pretraining than logistic regression, and the balanced accuracy was similar across models.
